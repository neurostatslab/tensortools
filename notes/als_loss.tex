\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{ahw_macros}
\usepackage[parfill]{parskip}

\title{Tricks to efficiently compute the objective function of CP tensor decomposition}
\author{Alex H Williams}
\date{March 2018}

\begin{document}

\maketitle

Consider a rank=$R$ canonical polyadic (CP) approximation of a tensor $\tensor{X} \in \R^{I \times J \times K}$. The model estimate, $\widehat{\tensor{X}}$ is defined by:
\begin{equation}
\label{eq:tensor-rep}
\widehat{\tensor{X}} = \sum_{r=1}^R \bu^r \circ \bv^r \circ \bw^r
\end{equation}
Where $\bU \in \R^{I \times R}$, $\bV \in \R^{J \times R}$, and $\bW \in \R^{K \times R}$ are factor matrices whose columns (indexed by $r$) are given by $\bu^r$, $\bv^r$, and $\bw^r$, respectively. We would like to compute the squared reconstruction error:
\[
\left \lVert \widehat{\tensor{X}} - \tensor{X} \right \lVert_F^2
\]

We can reformulate this into a matrix format by defining $\bX_{(3)} \in \R^{K \times IJ}$ as the \textit{unfolding} of the tensor along the third mode:
\begin{equation}
\label{eq:matrix-rep}
\left \lVert \bX_{(3)} - \bW (\bU \odot \bV)^T \right \lVert_F^2
\end{equation}
Where `$\odot$' operator denotes the Khatri-Rao product of two matrices.
This formulation is to update $\bW$ in the classic \textit{alternating least squares (ALS)} procedure.
In particular, on each iteration $\bW$ is updated according to:
\[
\bW \leftarrow \bX_{(3)} (\bU \odot \bV) \left ( (\bU \odot \bV)^T (\bU \odot \bV) \right )^{-1}
\]
Exploiting a property of the Khatri-Rao product, this update can be computed more efficiently:
\begin{equation}
\label{eq:update}
\bW \leftarrow \bX_{(3)} (\bU \odot \bV) \left ( \bU^T \bU * \bV^T \bV \right )^{-1}
\end{equation}
where `$*$' denotes the Hadamard product.
In ALS, $\bU$ and $\bV$ are updated in an analogous manner.

After updating $\bU$, $\bV$, and $\bW$ by the rule in equation 3, we will want to compute the reconstruction error to monitor convergence.
Na\"ively, computing equation \ref{eq:tensor-rep} by brute force will take $\mcO(NTK)$ operations.
We can get away with much less by caching the intermediate results given in the alternating least-squares algorithm.

We start by expanding equation \ref{eq:matrix-rep}:
\begin{align*}
&\phantom{=} \left \lVert \bX_{(3)} - \bW (\bU \odot \bV)^T \right \lVert_F^2 \\[.5em]
&= \underbrace{\Tr [ \bX_{(3)}^T \bX_{(3)} ]}_{(\#1)} - 2 \underbrace{\Tr [ \bX_{(3)} (\bU \odot \bV) \bW^T]}_{(\#2)} + \underbrace{\Tr [ (\bU \odot \bV) \bW^T \bW (\bU \odot \bV)^T ]}_{(\#3)}
\end{align*}

The first term $(\#1)$ is simply the squared norm of the data, which can be precomputed at the beginning of optimization. So this contributes no additional computational burden on each iteration.

In the second term, $(\#2)$, the most costly computation is the matrix multiplication, $\bX_{(3)} (\bU \odot \bV)$.
However, note that this term appears in the ALS update (eq. \ref{eq:update}), and so comes at no additional cost.
Let $\bC = \bX_{(3)} (\bU \odot \bV)$ denote this cached result.
Then, since the trace of a matrix product can be expressed in terms of their Hadamard product, we have:
\begin{equation}
\label{eq:hadamard}
\Tr [ \bX_{(3)} (\bU \odot \bV) \bW^T] = \Tr [ \bC \bW^T] = \sum_{i,j} \left [ \bC * \bW \right ]_{ij}
\end{equation}
Which takes $\mcO(KR)$ operations to compute.

The final term, $(\#3)$, can also be rearranged for rapid computation.
\begin{align*}
&\phantom{=} \Tr [ (\bU \odot \bV) \bW^T \bW (\bU \odot \bV)^T ] \\[.5em]
&= \Tr [ \bW^T \bW (\bU \odot \bV)^T (\bU \odot \bV)] \\[.5em]
&= \Tr [ \bW^T \bW (\bU^T \bU * \bV^T \bV)]
\end{align*}
Here, we applied the cyclic trace property and then the same property of the Khatri-Rao product used in equation \ref{eq:update}.
Note that $\bU^T \bU * \bV^T \bV$ is already computed in the ALS update, and therefore comes at no additional cost.
The Gramian $\bW^T \bW$ takes $\mcO(KR^2)$ operations to compute, which dominates the $\mcO(R^2)$ operations taken to compute the trace (we apply the same trick with the Hadamard product as in equation \ref{eq:hadamard}).

Overall, we reduced the time complexity from $\mcO(IJK)$ to $\mcO(KR^2)$ on each iteration.
Note that if $K > I$ or $K > J$ we can switch the order of the ALS updates to reduce the complexity further to either $\mcO(IR^2)$ or $\mcO(JR^2)$.

\end{document}
